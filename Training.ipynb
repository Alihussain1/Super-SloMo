{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "import math\n",
    "import UNet\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Details:\n",
    "![Architecture](img/Arch.png)\n",
    "## <font color='red' >Flow Computation Network:</font>\n",
    "* U-Net Architecture (in_channels = 6, out_Channels = 4)\n",
    "* input I0 , I1\n",
    "* output F0->1 , F1->0\n",
    "* taking two input images I0 and I1, to jointly predict the forward optical flow F0→1 and backward optical          flow F1→0 between them.\n",
    "        \n",
    "## <font color='red' >Arbitary-time flow interpolation:</font>\n",
    "* U-Net Architecture (in_channels = 20, out_Channels = 5)\n",
    "* inputs I1 , g(I1,Ft->1) , Ft->1, ft->0 , g(I0,Ft->0) , I0\n",
    "* outputs I1 , Vt<-1 , ▲Ft->1 , ▲Ft->0 , Vt<-0 , I0\n",
    "\n",
    "### I(t) is computed from Arbitart-time flow interpolation outputs\n",
    "\n",
    "# <font color='red' >Loss Function:</font>\n",
    "\n",
    "## <center><font color='blue' > L = λr lr + λp lp + λw lw + λs ls </font></center>\n",
    "* lr: Reconstruction loss to model how good the reconstruction of the intermediate frames\n",
    "* lp: Perceptual loss to preserve details of the predictions, and make interpolated frames sharper\n",
    "* lw: Wraping loss to model quality of computed optical flow\n",
    "* ls: Smoothness loss to encourage neighbboring pixels to have similir flow values\n",
    "* λr = 0.8 , λp = 0.005 , λw = 0.4 , λs = 1 \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "train_set_path = \"\"\n",
    "test_set_path = \"\"\n",
    "validation_set_path = \"\"\n",
    "\n",
    "image_seq_weights = np.linspace(0.125, 0.875, 7)\n",
    "\n",
    "batch_size = 6\n",
    "mean = [0.429, 0.431, 0.397]\n",
    "std  = [1, 1, 1]\n",
    "data_transform = transforms.Compose([    \n",
    "        transforms.Resize((352,352)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ])\n",
    "\n",
    "train_dataset = data_loader.dataset_loader(train_set_path,data_transform)\n",
    "validation_dataset = data_loader.dataset_loader(validation_set_path,data_transform)\n",
    "test_dataset = data_loader.dataset_loader(test_set_path,data_transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_model = UNet.UNet(6,4)\n",
    "arb_time_flow = UNet.UNet(20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = nn.L1Loss()\n",
    "percep_loss = nn.MSELoss()\n",
    "#loading Vgg16's conv_4_3 to use in loss calculation\n",
    "vgg16_model = torchvision.models.vgg16(pretrained=True)\n",
    "vgg16_conv_4_3 = nn.Sequential(*list(vgg16_model.children())[0][:22])\n",
    "for parameter in vgg16_conv_4_3.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# specify optimizer\n",
    "learning_parameters = list(flow_model.parameters()) + list(arb_time_flow.parameters())\n",
    "optimizer = optim.Adam(learning_parameters,lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediate_flow(F0_1,F1_0,t):\n",
    "    Ft_0 = (-(1-t)*t*F0_1) + (math.pow(t,2)  * F1_0)\n",
    "    Ft_1 = ( math.pow(1-t,2) * F0_1) - ( (1-t) * t * F1_0)\n",
    "    return Ft_0, Ft_1\n",
    "\n",
    "def apply_flow(img,flow,device):\n",
    "    \"\"\"\n",
    "        warp an image according to the optical flow\n",
    "        img : [B, C, H, W] ex: torch.Size([3, 3, 352, 352])\n",
    "        flow: [B, 2, H, W] ex: torch.Size([3, 2, 352, 352])\n",
    "        \n",
    "        F.grid_sample() : for input with shape (B, C, Hin, Win)\n",
    "        and grid with shape (B, Hout, Wout, 2), \n",
    "        the output will have shape (B, C, Hout, Wout).\n",
    "    \"\"\"\n",
    "    B,C,H,W = img.size()\n",
    "    gridx,gridy = np.meshgrid(np.linspace(-1,1,H),np.linspace(-1,1,W))\n",
    "    gridx = np.reshape(gridx,(1,1,H,W))\n",
    "    gridy = np.reshape(gridy,(1,1,H,W))\n",
    "    gridx = np.repeat(gridx,repeats=B,axis=0)\n",
    "    gridy = np.repeat(gridy,repeats=B,axis=0)\n",
    "    grid = np.concatenate((gridx,gridy),axis=1)\n",
    "    grid = torch.from_numpy(grid).float().to(device)\n",
    "    normalized_flow =  flow + grid\n",
    "    normalized_flow = normalized_flow.view(B,H,W,2)\n",
    "    new_img = F.grid_sample(img,normalized_flow,mode='bilinear')\n",
    "    return new_img\n",
    "\n",
    "def get_intermediate_image(t,Vt_0,Vt_1,g_I_0,g_I_1):\n",
    "    Z = (1-t)*Vt_0 + tVt_1\n",
    "    It = (1-t)*Vt_0 * g_I_0 \n",
    "    It += t*Vt_1 * g_I_1 \n",
    "    It *= (1/Z)\n",
    "    return It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "valid_loss_min = np.inf\n",
    "#training loop\n",
    "for i in range(epochs):\n",
    "    train_loss = 0\n",
    "    validation_loss = 0\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for train_data, frame_index in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        I0 ,It ,I1 = train_data\n",
    "        flow_out = flow_model(torch.cat((I0,I1),dim=1))\n",
    "        f0_1 = flow_out[:,:2,:,:]\n",
    "        f1_0 = flow_out[:,2:,:,:]\n",
    "        \n",
    "        ft_0_aprox ,ft_1_aprox = get_intermediate_flow(f0_1,f1_0,image_seq_weights[frame_index])\n",
    "        \n",
    "        g_I_0 = apply_flow(I0,ft_0_aprox,device)\n",
    "        g_I_1 = apply_flow(I1,ft_1_aprox,device)\n",
    "        \n",
    "        arbitary_time_flow = arb_time_flow(torch.cat((I0, I1, f0_1, f1_0, ft_1_aprox, ft_0_aprox, g_I_1, g_I_0), dim=1))\n",
    "        \n",
    "        ft_0 = arbitary_time_flow[:,:2,:,:] + ft_0_aprox\n",
    "        ft_1 = arbitary_time_flow[:,2:4,:,:]+ ft_1_aprox\n",
    "        vt_0 = arbitary_time_flow[:,4:5,:,:]\n",
    "        vt_1 = 1 - vt_0\n",
    "        \n",
    "        It_predicted = get_intermediate_image(image_seq_weights[frame_index],vt_0,vt_1,g_I_0,g_I_1)\n",
    "        \n",
    "        Lr = recon_loss(It,It_predicted)\n",
    "        Lp = percep_loss(vgg16_conv_4_3(It_predicted),vgg16_conv_4_3(It))\n",
    "        \n",
    "        g_I1_f01 = apply_flow(I1,f0_1)\n",
    "        g_I0_f10 = apply_flow(I0,f1_0)\n",
    "        Lw = recon_loss(I0,g_I1_f01) + recon_loss(I1,g_I0_f10) + percep_loss(It,g_I_0) + percep_loss(It,g_I_1)\n",
    "        \n",
    "        smooth_loss1_0 = torch.mean(torch.abs(f1_0[:, :, :, :-1] - f1_0[:, :, :, 1:])) + torch.mean(torch.abs(f1_0[:, :, :-1, :] - f1_0[:, :, 1:, :]))\n",
    "        smooth_loss0_1 = torch.mean(torch.abs(f0_1[:, :, :, :-1] - f0_1[:, :, :, 1:])) + torch.mean(torch.abs(f0_1[:, :, :-1, :] - f0_1[:, :, 1:, :]))\n",
    "        Ls = smooth_loss1_0 + smooth_loss0_1\n",
    "        \n",
    "        L = 0.8*Lr + 0.005*Lp + 0.4*Lw + Ls\n",
    "        # Backpropagate\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += L.item()\n",
    "    model.eval()\n",
    "    ##################################################################################################\n",
    "    for train_data, frame_index in valid_loader:\n",
    "        I0 ,It ,I1 = train_data\n",
    "        flow_out = flow_model(torch.cat((I0,I1),dim=1))\n",
    "        f0_1 = flow_out[:,:2,:,:]\n",
    "        f1_0 = flow_out[:,2:,:,:]\n",
    "        with torch.no_grad:\n",
    "            ft_0_aprox ,ft_1_aprox = get_intermediate_flow(f0_1,f1_0,image_seq_weights[frame_index])\n",
    "        \n",
    "            g_I_0 = apply_flow(I0,ft_0_aprox,device)\n",
    "            g_I_1 = apply_flow(I1,ft_1_aprox,device)\n",
    "        \n",
    "            arbitary_time_flow = arb_time_flow(torch.cat((I0, I1, f0_1, f1_0, ft_1_aprox, ft_0_aprox, g_I_1, g_I_0), dim=1))\n",
    "        \n",
    "            ft_0 = arbitary_time_flow[:,:2,:,:] + ft_0_aprox\n",
    "            ft_1 = arbitary_time_flow[:,2:4,:,:]+ ft_1_aprox\n",
    "            vt_0 = arbitary_time_flow[:,4:5,:,:]\n",
    "            vt_1 = 1 - vt_0\n",
    "        \n",
    "            It_predicted = get_intermediate_image(image_seq_weights[frame_index],vt_0,vt_1,g_I_0,g_I_1)\n",
    "        \n",
    "            Lr = recon_loss(It,It_predicted)\n",
    "            Lp = percep_loss(vgg16_conv_4_3(It_predicted),vgg16_conv_4_3(It))\n",
    "        \n",
    "            g_I1_f01 = apply_flow(I1,f0_1)\n",
    "            g_I0_f10 = apply_flow(I0,f1_0)\n",
    "            Lw = recon_loss(I0,g_I1_f01) + recon_loss(I1,g_I0_f10) + percep_loss(It,g_I_0) + percep_loss(It,g_I_1)\n",
    "        \n",
    "            smooth_loss1_0 = torch.mean(torch.abs(f1_0[:, :, :, :-1] - f1_0[:, :, :, 1:])) + torch.mean(torch.abs(f1_0[:, :, :-1, :] - f1_0[:, :, 1:, :]))\n",
    "            smooth_loss0_1 = torch.mean(torch.abs(f0_1[:, :, :, :-1] - f0_1[:, :, :, 1:])) + torch.mean(torch.abs(f0_1[:, :, :-1, :] - f0_1[:, :, 1:, :]))\n",
    "            Ls = smooth_loss1_0 + smooth_loss0_1\n",
    "        \n",
    "            L = 0.8*Lr + 0.005*Lp + 0.4*Lw + Ls\n",
    "            valid_loss += L.item()\n",
    "    ###################################################################################################\n",
    "    elabsed_time = time.time() - start_time\n",
    "    print('Epoch: {} \\tElabsed Time : {} Seconds\\nTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\\tValidation Accuracy: {}%'.format(\n",
    "                i+1, elabsed_time,train_loss, valid_loss, total_correct*100/len(valid_loader.dataset) ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        valid_loss_min = valid_loss\n",
    "        torch.save(flow_model.state_dict(), 'flow_model.pt')\n",
    "        torch.save(arb_time_flow.state_dict(), 'arb_time_flow.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
