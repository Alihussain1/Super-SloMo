{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import math\n",
    "import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Details:\n",
    "![Architecture](img/Arch.png)\n",
    "## <font color='red' >Flow Computation Network:</font>\n",
    "* U-Net Architecture (in_channels = 6, out_Channels = 4)\n",
    "* input I0 , I1\n",
    "* output F0->1 , F1->0\n",
    "* taking two input images I0 and I1, to jointly predict the forward optical flow F0→1 and backward optical          flow F1→0 between them.\n",
    "        \n",
    "## <font color='red' >Arbitary-time flow interpolation:</font>\n",
    "* U-Net Architecture (in_channels = 20, out_Channels = 5)\n",
    "* inputs I1 , g(I1,Ft->1) , Ft->1, ft->0 , g(I0,Ft->0) , I0\n",
    "* outputs I1 , Vt<-1 , ▲Ft->1 , ▲Ft->0 , Vt<-0 , I0\n",
    "\n",
    "### I(t) is computed from Arbitart-time flow interpolation outputs\n",
    "\n",
    "# <font color='red' >Loss Function:</font>\n",
    "\n",
    "## <center><font color='blue' > L = λr lr + λp lp + λw lw + λs ls </font></center>\n",
    "* lr: Reconstruction loss to model how good the reconstruction of the intermediate frames\n",
    "* lp: Perceptual loss to preserve details of the predictions, and make interpolated frames sharper\n",
    "* lw: Wraping loss to model quality of computed optical flow\n",
    "* ls: Smoothness loss to encourage neighbboring pixels to have similir flow values\n",
    "* λr = 0.8 , λp = 0.005 , λw = 0.4 , λs = 1 \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "batch_size = 6\n",
    "mean = [0.429, 0.431, 0.397]\n",
    "std  = [1, 1, 1]\n",
    "data_transform = transforms.Compose([    \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ])\n",
    "#train_dataset = \n",
    "#validation_dataset = \n",
    "#test_dataset = \n",
    "#train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "#validation_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size,shuffle=True)\n",
    "#test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_model = UNet.UNet(6,4)\n",
    "arb_time_flow = UNet.UNet(20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_loss = nn.L1Loss()\n",
    "percep_loss = nn.MSELoss()\n",
    "#loading Vgg16's conv_4_3 to use in loss calculation\n",
    "vgg16_model = torchvision.models.vgg16()#(pretrained=True)\n",
    "vgg16_conv_4_3 = nn.Sequential(*list(vgg16_model.children())[0][:22])\n",
    "for parameter in vgg16_conv_4_3.parameters():\n",
    "    parameter.requires_grad = False\n",
    "vgg16_conv_4_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.linspace(0.125, 0.875, 7)\n",
    "def get_intermediate_flow(F0_1,F1_0,weigths,index):\n",
    "    Ft_0 = (-(1-weigths[index])*weigths[index]*F0_1) + (math.pow(weigths[index],2)  * F1_0)\n",
    "    Ft_1 = ( math.pow(1-weigths[index],2) * F0_1) - ( (1-weigths[index]) * weigths[index] * F1_0)\n",
    "    return Ft_0, Ft_1\n",
    "def get_intermediate_image():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
